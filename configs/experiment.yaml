# Main experiment configuration
# This file contains hyperparameters, paths, and metadata for experiments

# Global settings
global:
  seed: 42
  device: "mps"  # auto, mps, cpu, cuda
  log_level: "INFO"

# Paths
paths:
  results_dir: "results"
  data_dir: "data"
  models_dir: "results/models"
  logs_dir: "results/logs"
  figures_dir: "results/figures"
  tables_dir: "results/tables"
  artifacts_dir: "results/artifacts"

# Reproducibility settings
reproducibility:
  deterministic: true
  benchmark: false
  tokenizers_parallelism: false
  cublas_workspace_config: ":4096:8"

# Training hyperparameters
training:
  # Overfitting parameters
  learning_rate: 1e-3
  num_epochs: 50
  weight_decay: 0.0
  dropout: 0.0
  early_stopping: false
  
  # Optimizer (use defaults)
  optimizer: "AdamW"
  
  # Gradient settings
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1

# Interpretability analysis
interpretability:
  # SHAP settings
  shap:
    max_samples: 100
    background_samples: 50
    explainer_type: "auto"  # auto, tree, deep, gradient
    device: "cpu"  # Always use CPU for SHAP
    
  # Metrics to compute
  metrics:
    faithfulness:
      enabled: true
      method: "removal"
      num_samples: 50
      
    consistency:
      enabled: true
      similarity_threshold: 0.7
      num_pairs: 100
      
    sparsity:
      enabled: true
      threshold: 0.01
      
    intuitiveness:
      enabled: false  # Requires human annotations
      annotation_file: null
      
  # Visualization settings
  visualization:
    max_features_display: 20
    figure_dpi: 300
    figure_format: "png"
    color_scheme: "viridis"
    save_plots: true

# Evaluation settings
evaluation:
  # Performance metrics
  performance_metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "auc_roc"
    - "auc_pr"
    - "confusion_matrix"
    
  # Cross-dataset evaluation
  cross_dataset:
    enabled: true
    strategy: "leave_one_dataset_out"
    compute_interpretability_consistency: true
    
  # Statistical testing
  statistical_tests:
    enabled: true
    significance_level: 0.05
    multiple_comparison_correction: "bonferroni"
    effect_size: true
    
  # Model comparison
  model_comparison:
    enabled: true
    baseline_model: "bag-of-words-tfidf"
    comparison_metrics:
      - "accuracy"
      - "f1"
      - "faithfulness"
      - "consistency"

# Experiment tracking
experiment_tracking:
  enabled: true
  backend: "local"  # local, mlflow, wandb
  
  # Experiment naming
  experiment_name: "nlp_interpretability_overfit"
  run_name_template: "{model}_{dataset}_{timestamp}"
  
  # Logging settings
  log_artifacts: true
  log_models: true
  log_metrics: true
  log_parameters: true

# Resource management
resources:
  max_memory_usage: 0.8
  num_workers: 0  # MPS compatibility
  max_training_time: 3600  # 1 hour per model
  max_evaluation_time: 1800  # 30 minutes per evaluation

# Logging settings
logging:
  console_level: "INFO"
  file_level: "DEBUG"
  log_experiments: true
  log_model_info: true
  log_dataset_info: true
  log_training_progress: true
  log_interpretability_results: true

# Output settings
output:
  # File naming conventions
  file_naming:
    model_checkpoint: "{model}_{dataset}_{epoch:03d}.pt"
    experiment_log: "{experiment}_{timestamp}.json"
    figure: "{model}_{dataset}_{metric}_{timestamp}.png"
    table: "{experiment}_{metric}_{timestamp}.csv"
    